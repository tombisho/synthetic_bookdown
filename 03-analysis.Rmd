# Analysis with synthetic data {#analysis}

In this section we describe how we can use a copy of the synthetic data to help users write DataSHIELD analysis code. Again we will make use of an existing data set.

Recall that the objective here is to use a synthetic copy of already harmonised real data that is then made available to the analyst on the client side. This synthetic data set can be loaded into **DSLite**, a special client side implementation of server side DataSHIELD used for development purposes. **DSLite** is will only take standard DataSHIELD functions as input, but the user can have full access to the data through a special interface. This is acceptable in this situation as the data are synthetic and designed to be non-disclosive. Therefore the user has the chance to write DataSHIELD code but see the complete results of each step. This makes it easier to develop the code.

The steps are summarised as:

1.	User requests synthetic copy of real data
2.	Synthetic data generated & available on client side
3.	Synthetic data placed in DSLite environment that simulates server side DataSHIELD environment
4.	Develop DataSHIELD code against synthetic data in DSLite (with full access to synthetic data)
5.	When DataSHIELD code is complete, run code against real data on server side



```{r echo=FALSE, fig.cap="Prototyping DataSHIELD analysis using synthetic data on DSLite"}
knitr::include_graphics(rep("images/dssynthetic_analysis.png"))

```

## Getting set up

Let us assume that the *DASIM1* dataset has previously been harmonised and is held on a server. As normal with DataSHIELD, we do not have complete access to the full data set but want to do some analysis on it. Using the method described above, the first thing to do is generate a synthetic version of the data that we can fully access. To do this we will use the same steps that were shown in \@ref(synthpop)

We build our log in object

```{r}
builder <- DSI::newDSLoginBuilder()
builder$append(server="server1", url="https://opal-sandbox.mrc-epid.cam.ac.uk",
               user="dsuser", password="P@ssw0rd", 
               table = "DASIM.DASIM1")
logindata <- builder$build()
```
Then perform the log in to the server:
```{r, message=FALSE}
library(DSOpal)
if(exists("connections")){
  datashield.logout(conns = connections)
}
connections <- datashield.login(logins=logindata, assign = TRUE)
```

## Create synthetic dataset
The *DASIM1* dataset is relatively small (i.e. around 10 columns). This is probably also true of many harmonised data sets.
Therefore we can just create a synthetic version of it in its entirety without specifying a subset of columns:

```{r, message=FALSE}
library(dsSyntheticClient)
synth_data = ds.syn(data = "D", method = "cart", m = 1, seed = 123)
# N.B. you may need to replace `server1` if you have named your connection differently
DASIM = synth_data$server1$syn
```

## Start DSLite local instance
Here we start our `DSLite` instance, and load the *DASIM* data. Recall that this simulates the server side environment on your client, but with the ability to access all the data. Therefore it is ideal for our requirement to build a code pipeline while being able to see the data: this helps with the debugging and logic checking process.
```{r, message=FALSE}
library(DSLite)
library(dsBaseClient)
dslite.server <- newDSLiteServer(tables=list(DASIM=DASIM))
dslite.server$config(defaultDSConfiguration(include=c("dsBase", "dsSynthetic")))

builder <- DSI::newDSLoginBuilder()
builder$append(server="server1", url="dslite.server", table = "DASIM", driver = "DSLiteDriver")
logindata <- builder$build()

if(exists("connections")){
  datashield.logout(conns = connections)
}
connections <- datashield.login(logins=logindata, assign = TRUE)
```

We can now check for ourselves that our *DASIM* data is in the DSLite server:

```{r, message=FALSE}
ds.summary('D')
```
## Write some analysis code
Now suppose we want to subset the *DASIM* data into men and women. We can use the `ds.subset` function:
```{r warning=FALSE, message=FALSE}
ds.subset(x="D", subset = "women", logicalOperator = "==", threshold = 1)
```
With DSLite we have the chance to look at actually what happened in detail:
```{r}
# N.B. you may need to replace `server1` if you have named your connection differently
women = getDSLiteData(conns = connections, symbol = "women")$server1
head(women)
```
This doesn't look quite right. There are still rows with `GENDER == 0`. There is an error in our code (we didn't specify `GENDER` as part of the logicalOperator parameter) but didn't get a warning. Let's make a correction and try again:
```{r warning=FALSE}
ds.subset(x="D", subset = "women", logicalOperator = "GENDER==", threshold = 1)
# get the data again
# N.B. you may need to replace `server1` if you have named your connection differently
women = getDSLiteData(conns = connections, symbol = "women")$server1
head(women)
```
This now looks much better.

## Run corrected code on real data

We can also compare results obtained via DataSHIELD with results on the actual data:

```{r, message=FALSE, warning=FALSE}
from_server = ds.glm(formula = "DIS_DIAB~PM_BMI_CONTINUOUS+LAB_TSC+LAB_HDL", data = "women", family = "binomial")
from_local = glm(formula = "DIS_DIAB~PM_BMI_CONTINUOUS+LAB_TSC+LAB_HDL", data = women, family = "binomial")

from_server$coefficients
from_local$coefficients
```
